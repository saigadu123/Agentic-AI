{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]= os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x000001C6EE16EEA0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C6EE16EFD0> root_client=<openai.OpenAI object at 0x000001C6EE20CF50> root_async_client=<openai.AsyncOpenAI object at 0x000001C6F51C8B90> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Agentic AI refers to artificial intelligence systems designed with agency, meaning they have the capacity to make autonomous decisions, pursue goals, and interact dynamically with their environment. These systems are often characterized by their ability to perceive their surroundings, make decisions based on those perceptions, and take actions to achieve specific objectives, sometimes even adapting their strategies based on feedback and new information.\\n\\nAgentic AI can range from simple autonomous systems, like basic robotics, to more complex entities capable of sophisticated decision-making processes. The design of such AI often involves elements of machine learning, decision theory, and sometimes even reinforcement learning, where the system learns from experience in a trial-and-error manner to optimize its performance over time.\\n\\nThe concept of agency in AI also raises important ethical and safety considerations, as systems with autonomous decision-making capabilities could potentially have significant impacts, both positive and negative, on their surrounding environment and human stakeholders. Thus, ensuring that agentic AI systems align with human values and societal norms is a critical area of ongoing research and discussion in the AI community.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 207, 'prompt_tokens': 13, 'total_tokens': 220, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-c0c1637f-c2a1-44d5-8dd1-0ba8ba2b6c22-0', usage_metadata={'input_tokens': 13, 'output_tokens': 207, 'total_tokens': 220, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is Agentic AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI engineer. Provide me answer based on the question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI engineer. Provide me answer based on the question.\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a suite of developer tools designed to enhance the development, testing, and deployment of applications utilizing Language Models (LLMs). It integrates with LLM applications, such as those built using LangChain, and offers features that streamline various stages of the LLM development lifecycle.\\n\\nKey components of Langsmith include:\\n\\n1. **Tracer**: This tool facilitates the tracking and debugging of processes within a chain or agent, enhancing transparency and accuracy.\\n\\n2. **Evaluation Framework**: Langsmith provides a framework for assessing LLM outputs, enabling comprehensive evaluation through LLM-assisted methods, labeled datasets, and preferred scoring metrics.\\n\\n3. **Dataset Handling**: It supports the efficient management of datasets for LLM application testing and evaluation.\\n\\n4. **Production Monitoring**: Real-time monitoring capabilities allow developers to keep track of their LLM applications in production, ensuring performance consistency and the ability to react quickly to potential issues.\\n\\nThese tools collectively aid developers in optimizing the creation of applications that rely on language models, ensuring robust performance and accurate results.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 208, 'prompt_tokens': 33, 'total_tokens': 241, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None} id='run-5840b4d9-aaf7-435f-b5ea-cdcbc3c0066a-0' usage_metadata={'input_tokens': 33, 'output_tokens': 208, 'total_tokens': 241, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a platform developed by LangChain, designed to facilitate the testing, debugging, and monitoring of applications built using large language models (LLMs). It provides extensive tools for developers to optimize their use of these models in applications, focusing on enhancing both performance and reliability.\n",
      "\n",
      "Key features of Langsmith include:\n",
      "\n",
      "1. **Testing and Debugging**: Langsmith offers capabilities to thoroughly test language models in various scenarios, enabling developers to identify and fix issues that may arise in LLM behavior.\n",
      "\n",
      "2. **Monitoring**: It provides insights into how language models are performing in real-time within applications, allowing developers to track key metrics and ensure the models are operating as expected.\n",
      "\n",
      "3. **Feedback Loops**: The platform helps establish feedback mechanisms to continuously improve model performance based on end-user interactions and other data inputs.\n",
      "\n",
      "4. **Fine-tuning and Optimization**: Langsmith facilitates ways to fine-tune models, helping developers tailor the performance of models to better meet specific application needs.\n",
      "\n",
      "Overall, Langsmith is designed to streamline the development process for applications using LLMs by providing a robust set of tools for handling the complexities associated with these advanced models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt| llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'overview': 'Langsmith is a platform or toolset designed to facilitate software development and enhance developer productivity. It often includes features such as code management, collaboration tools, and integrations with various development environments.', 'features': ['Code management', 'Collaboration tools', 'Integration with development environments', 'Enhanced developer productivity'], 'use_cases': ['Software development', 'Team collaboration', 'Project management'], 'benefits': ['Streamlined development process', 'Improved team communication', 'Efficient code management']}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables= [\"query\"],\n",
    "    partial_variables= {\"format_instructions\":output_parser.get_format_instructions()}\n",
    "\n",
    ")\n",
    "\n",
    "chain = prompt| llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"query\": \"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\nChains | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).CompositionChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL. LCEL is great for constructing your chains, but it's also nice to have chains used off the shelf. There are two types of off-the-shelf chains that LangChain supports:Chains that are built with LCEL. In this case, LangChain offers a higher-level constructor method. However, all that is being done under the hood is constructing a chain with LCEL. [Legacy] Chains constructed by subclassing from a legacy Chain class. These chains do not use LCEL under the hood but are the standalone classes.We are working on creating methods that create LCEL versions of all chains. We are doing this for a few reasons.Chains constructed in this way are nice because if you want to modify the internals of a chain you can simply modify the LCEL.These chains natively support streaming, async, and batch out of the box.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain constructors. Second, a list of all legacy Chains.LCEL Chains\\u200bBelow is a table of all LCEL chain constructors. Table columns:Chain Constructor: The constructor function for this chain. These are all methods that return LCEL Runnables. We also link to the API documentation.Function Calling: Whether this requires OpenAI function calling.Other Tools: Other tools (if any) used in this chain.When to Use: Our commentary on when to use this chain.Chain ConstructorFunction CallingOther ToolsWhen to Usecreate_stuff_documents_chainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using.create_openai_fn_runnable‚úÖIf you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for its call, but it does not have to call it.create_structured_output_runnable‚úÖIf you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.load_query_constructor_runnableCan be used to generate queries. You must specify a list of allowed operations and then return a runnable that converts a natural language query into those allowed operations.create_sql_query_chainSQL DatabaseIf you want to construct a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.create_retrieval_chainRetrieverThis chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a responseLegacy Chains\\u200bBelow are the legacy chains. We will maintain support for these until we create an LCEL alternative. Table columns:Chain: Name of the chain or name of the constructor method. If constructor method, this will return a Chain subclass.Function Calling: Whether chain requires OpenAI Function Calling.Other Tools: Other tools used in the chain.When to Use: Our commentary on when to use.ChainFunction CallingOther ToolsWhen to UseAPIChainRequests WrapperThis chain uses an LLM to convert a query into an API request, then executes that request, gets back a response, and then passes that request to an LLM to respondOpenAPIEndpointChainOpenAPI SpecSimilar to APIChain, this chain is designed to interact with APIs. The main difference is this is optimized for ease of use with OpenAPI endpointsConversationalRetrievalChainRetrieverThis chain can be used to have conversations with a document. It takes in a question and (optional) previous conversation history. If there is a previous conversation history, it uses an LLM to rewrite the conversation into a query to send to a retriever (otherwise it just uses the newest user input). It then fetches those documents and passes them (along with the conversation) to an LLM to respond.StuffDocumentsChainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using.ReduceDocumentsChainThis chain combines documents by iterative reducing them. It groups documents into chunks (less than some context length) and then passes them into an LLM. It then takes the responses and continues to do this until it can fit everything into one final LLM call. It is useful when you have a lot of documents, you want to have the LLM run over all of them, and you can do it in parallel.MapReduceDocumentsChainThis chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. It is useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents.RefineDocumentsChainThis chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to refine its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls).MapRerankDocumentsChainThis calls on LLM on each document, asking it to not only answer but also produce a score of how confident it is. The answer with the highest confidence is then returned. This is useful when you have a lot of documents, but only want to answer based on a single document, rather than trying to combine answers (like Refine and Reduce methods do).ConstitutionalChainThis chain answers, then attempts to refine its answer based on constitutional principles that are provided. Use this to enforce that a chain's answer follows some principles.LLMChainElasticsearchDatabaseChainElasticsearch InstanceThis chain converts a natural language question to an Elasticsearch query, and then runs it, and then summarizes the response. This is useful for when you want to ask natural language questions of an Elasticsearch databaseFlareChainThis implements FLARE, an advanced retrieval technique. It is primarily meant as an exploratory advanced retrieval method.ArangoGraphQAChainArango GraphThis chain constructs an Arango query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphCypherQAChainA graph that works with Cypher query languageThis chain constructs a Cypher query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.FalkorDBGraphQAChainFalkor DatabaseThis chain constructs a FalkorDB query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.HugeGraphQAChainHugeGraphThis chain constructs an HugeGraph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.KuzuQAChainKuzu GraphThis chain constructs a Kuzu Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NebulaGraphQAChainNebula GraphThis chain constructs a Nebula Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NeptuneOpenCypherQAChainNeptune GraphThis chain constructs a Neptune Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphSparqlChainGraph that works with SparQLThis chain constructs a SparQL query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.LLMMathThis chain converts a user question to a math problem and then executes it (using numexpr)LLMCheckerChainThis chain uses a second LLM call to verify its initial answer. Use this when you have an extra layer of validation on the initial LLM call.LLMSummarizationCheckerThis chain creates a summary using a sequence of LLM calls to make sure it is extra correct. Use this over the normal summarization chain when you are okay with multiple LLM calls (eg you care more about accuracy than speed/cost).create_citation_fuzzy_match_chain‚úÖUses OpenAI function calling to answer questions and cite its sources.create_extraction_chain‚úÖUses OpenAI Function calling to extract information from text.create_extraction_chain_pydantic‚úÖUses OpenAI function calling to extract information from text into a Pydantic model. Compared to create_extraction_chain this has a tighter integration with Pydantic.get_openapi_chain‚úÖOpenAPI SpecUses OpenAI function calling to query an OpenAPI.create_qa_with_structure_chain‚úÖUses OpenAI function calling to do question answering over text and respond in a specific format.create_qa_with_sources_chain‚úÖUses OpenAI function calling to answer questions with citations.QAGenerationChainCreates both questions and answers from documents. Used to generate question/answer pairs for evaluation of retrieval projects.RetrievalQAWithSourcesChainRetrieverDoes question answering over retrieved documents, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over load_qa_with_sources_chain when you want to use a retriever to fetch the relevant document as part of the chain (rather than pass them in).load_qa_with_sources_chainRetrieverDoes question answering over documents you pass in, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over RetrievalQAWithSources when you want to pass in the documents directly (rather than rely on a retriever to get them).RetrievalQARetrieverThis chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a response.MultiPromptChainThis chain routes input between multiple prompts. Use this when you have multiple potential prompts you could use to respond and want to route to just one.MultiRetrievalQAChainRetrieverThis chain routes input between multiple retrievers. Use this when you have multiple potential retrievers you could fetch relevant documents from and want to route to just one.EmbeddingRouterChainThis chain uses embedding similarity to route incoming queries.LLMRouterChainThis chain uses an LLM to route between potential options.load_summarize_chainLLMRequestsChainThis chain constructs a URL from user input, gets data at that URL, and then summarizes the response. Compared to APIChain, this chain is not focused on a single API spec but is more generalHelp us out by providing feedback on this documentation page:PreviousTimeouts for agentsNext[Beta] MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.1/docs/modules/chains/\")\n",
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='Chains | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content=\"Skip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).CompositionChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL. LCEL is great for constructing your chains, but it's also nice to have chains used off the shelf. There are two types of off-the-shelf chains\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content=\"step. The primary supported way to do this is with LCEL. LCEL is great for constructing your chains, but it's also nice to have chains used off the shelf. There are two types of off-the-shelf chains that LangChain supports:Chains that are built with LCEL. In this case, LangChain offers a higher-level constructor method. However, all that is being done under the hood is constructing a chain with LCEL. [Legacy] Chains constructed by subclassing from a legacy Chain class. These chains do not use LCEL under the hood but are the standalone classes.We are working on creating methods that create LCEL versions of all chains. We are doing this for a few reasons.Chains constructed in this way are nice because if you want to modify the internals of a chain you can simply modify the LCEL.These chains natively support streaming, async, and batch out of the box.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain constructors.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='natively support streaming, async, and batch out of the box.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain constructors. Second, a list of all legacy Chains.LCEL Chains\\u200bBelow is a table of all LCEL chain constructors. Table columns:Chain Constructor: The constructor function for this chain. These are all methods that return LCEL Runnables. We also link to the API documentation.Function Calling: Whether this requires OpenAI function calling.Other Tools: Other tools (if any) used in this chain.When to Use: Our commentary on when to use this chain.Chain ConstructorFunction CallingOther ToolsWhen to Usecreate_stuff_documents_chainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using.create_openai_fn_runnable‚úÖIf you want to use OpenAI function calling to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using.create_openai_fn_runnable‚úÖIf you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for its call, but it does not have to call it.create_structured_output_runnable‚úÖIf you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.load_query_constructor_runnableCan be used to generate queries. You must specify a list of allowed operations and then return a runnable that converts a natural language query into those allowed operations.create_sql_query_chainSQL DatabaseIf you want to construct a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.create_retrieval_chainRetrieverThis chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a responseLegacy Chains\\u200bBelow are the legacy chains. We will maintain support for these until we create an LCEL alternative. Table columns:Chain: Name of the chain or name of the constructor method. If constructor method, this will return a Chain subclass.Function Calling: Whether chain requires OpenAI Function Calling.Other Tools: Other tools used in the chain.When to Use: Our commentary on when to use.ChainFunction CallingOther ToolsWhen to UseAPIChainRequests WrapperThis chain uses an LLM to convert a query into an API request, then executes that request, gets'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='Use: Our commentary on when to use.ChainFunction CallingOther ToolsWhen to UseAPIChainRequests WrapperThis chain uses an LLM to convert a query into an API request, then executes that request, gets back a response, and then passes that request to an LLM to respondOpenAPIEndpointChainOpenAPI SpecSimilar to APIChain, this chain is designed to interact with APIs. The main difference is this is optimized for ease of use with OpenAPI endpointsConversationalRetrievalChainRetrieverThis chain can be used to have conversations with a document. It takes in a question and (optional) previous conversation history. If there is a previous conversation history, it uses an LLM to rewrite the conversation into a query to send to a retriever (otherwise it just uses the newest user input). It then fetches those documents and passes them (along with the conversation) to an LLM to respond.StuffDocumentsChainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='documents and passes them (along with the conversation) to an LLM to respond.StuffDocumentsChainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using.ReduceDocumentsChainThis chain combines documents by iterative reducing them. It groups documents into chunks (less than some context length) and then passes them into an LLM. It then takes the responses and continues to do this until it can fit everything into one final LLM call. It is useful when you have a lot of documents, you want to have the LLM run over all of them, and you can do it in parallel.MapReduceDocumentsChainThis chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. It is useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents.RefineDocumentsChainThis'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='reduces them using the ReduceDocumentsChain. It is useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents.RefineDocumentsChainThis chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to refine its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls).MapRerankDocumentsChainThis calls on LLM on each document, asking it to not only answer but also produce a score of how confident it is. The answer with the highest confidence is then returned. This is useful when you have a lot of documents, but only want to answer based on a single document, rather than trying to combine answers (like Refine and Reduce methods do).ConstitutionalChainThis chain answers,'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content=\"when you have a lot of documents, but only want to answer based on a single document, rather than trying to combine answers (like Refine and Reduce methods do).ConstitutionalChainThis chain answers, then attempts to refine its answer based on constitutional principles that are provided. Use this to enforce that a chain's answer follows some principles.LLMChainElasticsearchDatabaseChainElasticsearch InstanceThis chain converts a natural language question to an Elasticsearch query, and then runs it, and then summarizes the response. This is useful for when you want to ask natural language questions of an Elasticsearch databaseFlareChainThis implements FLARE, an advanced retrieval technique. It is primarily meant as an exploratory advanced retrieval method.ArangoGraphQAChainArango GraphThis chain constructs an Arango query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphCypherQAChainA graph that works with Cypher\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='chain constructs an Arango query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphCypherQAChainA graph that works with Cypher query languageThis chain constructs a Cypher query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.FalkorDBGraphQAChainFalkor DatabaseThis chain constructs a FalkorDB query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.HugeGraphQAChainHugeGraphThis chain constructs an HugeGraph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.KuzuQAChainKuzu GraphThis chain constructs a Kuzu Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NebulaGraphQAChainNebula GraphThis chain constructs a Nebula Graph query from'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NebulaGraphQAChainNebula GraphThis chain constructs a Nebula Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NeptuneOpenCypherQAChainNeptune GraphThis chain constructs a Neptune Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphSparqlChainGraph that works with SparQLThis chain constructs a SparQL query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.LLMMathThis chain converts a user question to a math problem and then executes it (using numexpr)LLMCheckerChainThis chain uses a second LLM call to verify its initial answer. Use this when you have an extra layer of validation on the initial LLM call.LLMSummarizationCheckerThis chain creates'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='chain uses a second LLM call to verify its initial answer. Use this when you have an extra layer of validation on the initial LLM call.LLMSummarizationCheckerThis chain creates a summary using a sequence of LLM calls to make sure it is extra correct. Use this over the normal summarization chain when you are okay with multiple LLM calls (eg you care more about accuracy than speed/cost).create_citation_fuzzy_match_chain‚úÖUses OpenAI function calling to answer questions and cite its sources.create_extraction_chain‚úÖUses OpenAI Function calling to extract information from text.create_extraction_chain_pydantic‚úÖUses OpenAI function calling to extract information from text into a Pydantic model. Compared to create_extraction_chain this has a tighter integration with Pydantic.get_openapi_chain‚úÖOpenAPI SpecUses OpenAI function calling to query an OpenAPI.create_qa_with_structure_chain‚úÖUses OpenAI function calling to do question answering over text and respond in a specific'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='SpecUses OpenAI function calling to query an OpenAPI.create_qa_with_structure_chain‚úÖUses OpenAI function calling to do question answering over text and respond in a specific format.create_qa_with_sources_chain‚úÖUses OpenAI function calling to answer questions with citations.QAGenerationChainCreates both questions and answers from documents. Used to generate question/answer pairs for evaluation of retrieval projects.RetrievalQAWithSourcesChainRetrieverDoes question answering over retrieved documents, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over load_qa_with_sources_chain when you want to use a retriever to fetch the relevant document as part of the chain (rather than pass them in).load_qa_with_sources_chainRetrieverDoes question answering over documents you pass in, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over RetrievalQAWithSources when you'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='question answering over documents you pass in, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over RetrievalQAWithSources when you want to pass in the documents directly (rather than rely on a retriever to get them).RetrievalQARetrieverThis chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a response.MultiPromptChainThis chain routes input between multiple prompts. Use this when you have multiple potential prompts you could use to respond and want to route to just one.MultiRetrievalQAChainRetrieverThis chain routes input between multiple retrievers. Use this when you have multiple potential retrievers you could fetch relevant documents from and want to route to just one.EmbeddingRouterChainThis chain uses embedding similarity to route incoming queries.LLMRouterChainThis chain uses an LLM to route between potential'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='documents from and want to route to just one.EmbeddingRouterChainThis chain uses embedding similarity to route incoming queries.LLMRouterChainThis chain uses an LLM to route between potential options.load_summarize_chainLLMRequestsChainThis chain constructs a URL from user input, gets data at that URL, and then summarizes the response. Compared to APIChain, this chain is not focused on a single API spec but is more generalHelp us out by providing feedback on this documentation page:PreviousTimeouts for agentsNext[Beta] MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(document)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='297cfd10-0b27-416d-af40-f9c27254b3ed', metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content=\"Skip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).CompositionChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL. LCEL is great for constructing your chains, but it's also nice to have chains used off the shelf. There are two types of off-the-shelf chains\"),\n",
       " Document(id='892ea969-dd5b-4360-97fe-df3e50169ec8', metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='documents from and want to route to just one.EmbeddingRouterChainThis chain uses embedding similarity to route incoming queries.LLMRouterChainThis chain uses an LLM to route between potential options.load_summarize_chainLLMRequestsChainThis chain constructs a URL from user input, gets data at that URL, and then summarizes the response. Compared to APIChain, this chain is not focused on a single API spec but is more generalHelp us out by providing feedback on this documentation page:PreviousTimeouts for agentsNext[Beta] MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(id='1abc38f6-0e45-49e0-ad67-55c4bd14ebb4', metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='chain constructs an Arango query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphCypherQAChainA graph that works with Cypher query languageThis chain constructs a Cypher query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.FalkorDBGraphQAChainFalkor DatabaseThis chain constructs a FalkorDB query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.HugeGraphQAChainHugeGraphThis chain constructs an HugeGraph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.KuzuQAChainKuzu GraphThis chain constructs a Kuzu Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NebulaGraphQAChainNebula GraphThis chain constructs a Nebula Graph query from'),\n",
       " Document(id='17656d40-1cf3-46c6-8db3-b535d26b73c8', metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/chains/', 'title': 'Chains | ü¶úÔ∏èüîó LangChain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.', 'language': 'en'}, page_content='question answering over documents you pass in, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over RetrievalQAWithSources when you want to pass in the documents directly (rather than rely on a retriever to get them).RetrievalQARetrieverThis chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a response.MultiPromptChainThis chain routes input between multiple prompts. Use this when you have multiple potential prompts you could use to respond and want to route to just one.MultiRetrievalQAChainRetrieverThis chain routes input between multiple retrievers. Use this when you have multiple potential retrievers you could fetch relevant documents from and want to route to just one.EmbeddingRouterChainThis chain uses embedding similarity to route incoming queries.LLMRouterChainThis chain uses an LLM to route between potential')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"This application will translate text from English to another language\"\n",
    "\n",
    "result = vectorstore.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "      \"\"\" \n",
    "    Answer the following question based on the provided context.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "\n",
    "   \"\"\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
